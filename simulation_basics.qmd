# Simulation basics {#sec-simulation-basics}

## What is Simulation?

In science and engineering, it is of paramount importance to develop reliable quantitative models
 that capture the essential behavior of real systems. Simulation provides a 
 controlled, repeatable, and cost‑effective way to

- predict system behavior under varied conditions,
- explore "what‑if" scenarios and design alternatives,
- quantify uncertainty and sensitivity to inputs,
- validate hypotheses when experiments are impractical or expensive,
- and support optimization and decision making.

A simulation study typically involves the following steps:

1. Construct a mathematical or computational model.
2. Specify inputs and assumptions. 
3. Run experiments (often many replications with different parameters).
4. Analyze outputs and comparing them with data or theoretical expectations. 

Proper validation and uncertainty quantification are critical to ensure that simulation results are trustworthy and useful for engineering practice.

Simulation can be defined as the methods and procedures to define models of a system of interest and execute it
to get raw data [@osais_computer_2017]. In normal simulation studies, we are not interested in the raw data by itself, but use it to calculate measures of interest regarding the system's performance. For instance, in the example
shown in @sec-introduction, we saw that measures of interest include the average time that a customer 
has to way in the checkout queue. We sometimes also call these raw data *synthetic data*, since this is 
not the actual data that we would collect in the physical world. Synthetic data has by itself sparked 
interest in recent years due to its potential to enhance how we train and validate machine learning models, 
especially regarding data privacy and robustness, or when training data is expensive or scarce
[@jordon_synthetic_2022; @van_breugel_synthetic_2023].

In the rest of this chapter, we will introduce the basic principles and notions needed to understand
how simulation works. We start with a gentle reminder of random numbers and distributions, and introduce 
standard methods of random number generation. We then move on into stochastic processes and how
discrete-event simulation works. After that, we present common statistical techniques to deal with the
output data of simulations and conclude the chapter with considerations about verification and validation 
of simulation studies.

## Dealing with Random Numbers

We refer to *random numbers* as realizations of random variables that follow probability distributions. The 
following elements completely determine the statistical behaviour of randon numbers:

- Their **type**: discrete or continuous?
- The form of their **probability distribution**: binomial, normal, exponential, Poisson, etc.
- The **joint** or **conditional** distributions associated with the phenomenon at hand.
- The specific **parameters** used for each probability distribution.

In this book, we will mainly deal with parametric probability distributions, although everything applies 
to non-parametric distributions as well. We will hint at specific differences when appropriate.

### Pseudorandom Number Generators

In general, any procedure to generate random numbers is called a *pseudorandom number generator* (PRNB).
A PRNB can be defined as a deterministic algorithm that, given an initial seed, produces a long sequence of numbers that mimic the statistical properties of truly random samples. Although the sequence is fully determined by the seed (so it is not truly random, hence *pseudorandom*), a good PRNG yields values that are uniformly distributed, have minimal serial correlation, and pass standard statistical tests. Important PRNG properties include period length, equidistribution, independence, speed, and reproducibility (the same seed reproduces the same sequence). For simulation work we typically prefer generators with very long periods and strong statistical quality while cryptographic applications require cryptographically secure PRNGs. PRNGs are used to produce uniform variates that are then transformed into other distributions via methods such as inverse transform sampling, acceptance–rejection, or composition.

Let's explore the properties of a specific PRNG, the Linear Congruential Generator (LCG) using the following Python code.

```{python}
import numpy as np
from scipy.stats import chisquare
from collections import defaultdict


class LCG:
    """
    X(n+1) = (a * X(n) + c) mod m
    """
    def __init__(self, seed, a, c, m):
        self._state = seed
        self.a = a
        self.c = c
        self.m = m
        self.seed = seed

    def next_int(self):
        """Generates the next pseudo-random integer 
        in the sequence."""
        self._state = (self.a * self._state + self.c) % self.m
        return self._state

    def generate(self, size):
        """Generates a sequence of integers and 
        normalizes them to [0, 1)."""
        sequence_int = []
        sequence_float = []
        # Reset state to seed for sequence generation
        self._state = self.seed 
        
        for _ in range(size):
            next_val = self.next_int()
            sequence_int.append(next_val)
            # Normalize to a float in [0, 1) by dividing by the modulus
            sequence_float.append(next_val / self.m)
            
        return np.array(sequence_int), np.array(sequence_float)

```

The LCG is one of the oldest and best known PRNG which are used to date. As can be seen in 
the code, it uses three integer parameters $a$, $c$ and the modulo $m$ and computes the 
next random number using the recurrence:

$$
X_{n+1} = (a X_n + c) \operatorname{mod} m
$$ {#eq-1}

Starting at $n=0$, we initialize $X_0$ to the random seed provided.

We can now use the generator as follows:

```{python}
# LCG Parameters (a 'poor' LCG to highlight the deterministic nature)
# A small modulus (m) leads to a short period and visible patterns.
SEED = 42
A = 65  # Multiplier
C = 1   # Increment
M = 2**10  # Modulus (1024) - A small M is used for demonstration purposes
SEQUENCE_SIZE = 100000

# 1. Initialize and Generate Sequence
prng = LCG(SEED, A, C, M)
int_sequence, float_sequence = prng.generate(SEQUENCE_SIZE)
int_sequence[:10]
```

We have now generated 100000 random numbers using LCG (only first 10 are shown). But how can we ensure 
if this PRNM works well in practice? We will look now at the **period length**, how to check for **uniformity**
and how to assert if there is **serial correlation**.

**Period length**

The period length assesses the number of values generated before the sequence of states returns to the first 
value (the starting state) for the first time. Note that in general the longer, the better. Note that 
in this case, the maximum possible period is $m$, the modulo of the generator. We can calculate this with a simple Python function as follows:

```{python}
def calculate_period(lcg_generator):
    """
    Calculates the period (cycle length) of the LCG.
    The period is the number of values generated before the sequence repeats.
    """
    initial_state = lcg_generator.seed
    current_state = initial_state
    
    # Check for the next state immediately after the seed to start the loop
    current_state = (lcg_generator.a * current_state + lcg_generator.c) % lcg_generator.m
    period = 1
    
    # Loop until the state returns to the initial seed
    while current_state != initial_state:
        current_state = (lcg_generator.a * current_state + lcg_generator.c) % lcg_generator.m
        period += 1
        
        # Safety break for potentially infinite loops in case of a non-standard LCG
        if period > lcg_generator.m:
           return f"Period is greater than modulus m ({lcg_generator.m}). Check parameters."
            
    return period

period = calculate_period(prng)
period
```

So in this case, our generator reaches the maximum period (1024), which is the best we can do.

**Tests for uniformity**

We want the generated random numbers to be uniformly generated (we will see later how generate numbers with 
different distributions started with uniformly generated random numbers). For this, we use the $\chi^2$ test
for uniformity:

- Null Hypothesis ($H_0$): The generated numbers are uniformly distributed.
- Alternative Hypothesis ($H_1$): The generated numbers are not uniformly distributed.

The main idea of this test is to divide the generated numbers in intervals, and check whether those
intervals contain roughly the same number of generated values (e.g. a flat histogram). Like in the classical 
$\chi^2$ test, we calculate the expected $E_i$ and the observed $O_i$ frequencies for each range and calculate the $\chi^2$ statistic as usual:

$$
\chi^2=\sum_{i=1}^k\frac{(O_i-E_i)^2}{E_i}
$$

We can use the following Python function:

```{python}
def chi_squared_uniformity_test(data_float, num_bins=10):
    """
    Statistical Test: Chi-Squared Goodness-of-Fit Test for Uniformity.
    """
    N = len(data_float)
    
    # 1. Bin the data to get observed frequencies
    # The bins are equal-sized intervals in [0, 1).
    observed_frequencies, _ = np.histogram(data_float, bins=num_bins, range=(0, 1))
    
    # 2. Calculate expected frequencies for a perfectly uniform distribution
    expected_frequency = N / num_bins
    expected_frequencies = np.full(num_bins, expected_frequency)
    
    # 3. Perform the Chi-Squared test
    # The 'chisquare' function compares observed and expected frequencies.
    # A small p-value (e.g., < 0.05) leads to rejection of H0, meaning non-uniformity.
    chi2_stat, p_value = chisquare(f_obs=observed_frequencies, f_exp=expected_frequencies)
    
    return chi2_stat, p_value, num_bins

chi2_stat, p_value_uniformity, num_bins = chi_squared_uniformity_test(float_sequence)
print(f'Chi2 statistic: {chi2_stat}, p-value: {p_value_uniformity}, number of bins: {num_bins}')
```

In this case, the p-value is much higher than $\alpha=0.05$ and we **cannot** reject $H_0$, so 
the numbers appear to be uniformly random.

**Serial correlation**

The next possible measure to check is the serial correlation between the numbers generated. 
The **Serial Correlation Check**, also known as **Autocorrelation at Lag 1**, is a diagnostic measure used to characterize 
and detect a fundamental weakness in simple Pseudorandom Number Generators (PRNGs), such as the Linear Congruential Generator (LCG).
The main idea is that the correlation between immediately adjacent numbers (hence lag 1) should be zero.

To calculate this, we form two sequences: the generated numbers and the same sequence moved by one place:

$$
\begin{aligned}
S_n & = \{X_1,X_2,X_3,\dots,X_{n-1}\} \\
S_{n+1} & = \{X_2,X_3,X_4,\dots,X_n\}
\end{aligned}
$$

And now we calculate the Pearson correlation coefficient between $S_1$ and $S_2$.

$$
r=\frac{\sum (S_1-\bar{S_1})(S_2-\bar{S_2})}{\sqrt{\sum (S_1-\bar{S_1})^2(S_2-\bar{S_2})^2}}
$$

Our goal is that $r$ is as close to zero as possible (note that $r\in[-1,1]$). Let's use 
the following code:

```{python}
def serial_correlation_check(data_float):
    """
    Characterization: Autocorrelation (Serial Correlation) Check.
    """
    # X_n: all values except the last one
    X_n = data_float[:-1]
    # X_{n+1}: all values except the first one
    X_n_plus_1 = data_float[1:]
    
    # Calculate the Pearson correlation coefficient (r)
    # The result is an array, we take the correlation between the two sequences (index 0, 1)
    correlation_matrix = np.corrcoef(X_n, X_n_plus_1)
    lag_1_correlation = correlation_matrix[0, 1]
    
    return lag_1_correlation

lag_1_correlation = serial_correlation_check(float_sequence)
print(f'The lag 1 correlation coefficient is {lag_1_correlation}')
```
While the value is low, it's not as close to zero as it should, which is a known 
weakness of the LCG (the generated numbers tend to fall onto a number of parallel 
hyperplanes). This is the reason why PRNM like the LCG are not normally used in practice.
The de-facto standard for pseudorandom number generation in practice is the algorithm
known as the **Mersenne Twister**. This is the default generator used in Python or MATLAB, 
and the preferred one for simulation purposes (but *not* for cryptographic purposes). The 
basic idea is to use a highly non-linear twisted generalized feedback shift register. 
Apart from being much faster than LCG, it passes the serial correlation chek with flying colors:

```{python}
import random

random.seed(SEED)

# Generate a sequence of random floats in the range [0.0, 1.0)
float_sequence_mt = np.array([random.uniform(0, 1) for _ in range(SEQUENCE_SIZE)])

# Serial Correlation Check
lag_1_correlation_mt = serial_correlation_check(float_sequence_mt)
print(f'The lag 1 correlation coefficient is {lag_1_correlation_mt}')

```
which is an order of magnitude better than the LCG.

## Sampling Methods

We have now a method for generating *uniformly distributed* random numbers. But what about other 
widely used distributions, like normal, exponential, Poisson, etc? In this section, we will review 
three popular methods for this purpose: the **inversion method**, the **rejection sampling** method, the **Box-Muller transform** and 
the **mixture method**. For all three methods, the general problem is as follows: we start 
with a random variable $U \sim \text{ Uniform}(0,1)$. We want to convert $U$ into $X \sim f(x)$, 
where $f$ is the target PDF of $X$.



**Inversion method**

Suppose that we know the CDF of the target distribution $F(x)=P(X\le x)$, and assume that 
we can invert it to $F^{-1}(u)$. With this function, we can simply obtain $X$ by

$$
X=F^{-1}(U)
$$ {#eq-inv}

For instance, imagine our target distribution is the exponential, with density function
$f(x)=\lambda e^{-\lambda x}$. Elementary calculus tells us that $F(x)=1-e^{\lambda x}$. 
It can be shown that the inverse is

$$
F^{-1}(u)=-\frac{1}{\lambda}\ln(1-u)
$$ {#eq-example-inv}

Since $U'=1-U$ is also uniform in $[0,1]$, we can simply write $X=-\frac{1}{\lambda}\ln(U')$.

**Rejection-sampling method**

But what if our CDF is not easily invertible, or worse, we don't have any analytical expression
for it? Suppose that, although we don't have $f$, we have a proposal distribution $g(x)$ so that 
it "envelopes" the target distribution in the sense that there is a constant $c$ so 
that $f(x)\le c g(x)$ (i.e., we **do** know the PDF). In this case, we can do the following:

1. Sample $x$ from the proposal distribution $g(x)$.
2. Sample a uniform $U(0,1)$ random variable $u$.
3. If $u < \frac{f(x)}{c g(x)}$, the candidate number $x$ is accepted since it follows $f(x)$.
4. Otherwise, we repeat the procedure until we get a candidate accepted.

The trick now is to take a *bounded* uniform distribution as $g$ that contains our target 
distribution $f$. Once we have this, we can generate samples from virtually any 
probability distribution without requiring its CDF or inverse.

**Box-Muller transform**

The next method is specialized towards generating values for the **normal distribution**, and 
is widely used in practice. We start by generating two uniform random numbers $u_1, u_2 \sim U(-1,1)$.

* First, we calculte the sum of their squares $S=u_1^2+u_2^2$.
* If $S\ge 1$ or $S=0$, we reject both and return to the first step.
* Otherwise, we calculate $C=\sqrt{\frac{-2\ln(S)}{S}}$.
* We output two normally distributed random numbers as $z_1=u_1 C$ and $z_2 = u_2 C$.

The random numbers generated follow a standard normal distribution $N(0,1)$. For an arbitrary
normal distribution $N(\mu,\sigma^2)$ we just scale using the standard transformation $X=\mu+\sigma Z$.

**Mixture method**

In the case that the target distribution can be expressed as a mixture of $k$ different PDFs

$$
f(x)=\sum_{i=1}^k p_i f_i(x)\text{, with }p_i\ge 0\text{, and }\sum_{i=1}^k p_i = 1
$$ {#eq-mixtures}

Then we can use the following methods to sample from $f(x)$:

* Choose randomly an index $i\in I$ from the set of indices $I=\{1,2,\dots,k\}$. This is done by
generating a random number $u \sim U(0,1)$ and choosing the least index $j$ so that 
$\sum_{i=1}^j p_i < u$.
* Generate a random variable $x$ from $f_i(x)$ by using any of the aforementioned methods.

This is a suitable method, for instance, to generate random numbers that follow a 
**Gaussian Mixture Model (GMM)**. In this case, we just sample an index and generate a 
random number according to the Box-Muller method, scaling accordingly if necessary.

