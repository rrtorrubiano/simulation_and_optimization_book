# Simulation basics {#sec-simulation-basics}

## What is Simulation?

In science and engineering, it is of paramount importance to develop reliable quantitative models
 that capture the essential behavior of real systems. Simulation provides a 
 controlled, repeatable, and cost‑effective way to

- predict system behavior under varied conditions,
- explore "what‑if" scenarios and design alternatives,
- quantify uncertainty and sensitivity to inputs,
- validate hypotheses when experiments are impractical or expensive,
- and support optimization and decision making.

A simulation study typically involves the following steps:

1. Construct a mathematical or computational model.
2. Specify inputs and assumptions. 
3. Run experiments (often many replications with different parameters).
4. Analyze outputs and comparing them with data or theoretical expectations. 

Proper validation and uncertainty quantification are critical to ensure that simulation results are trustworthy and useful for engineering practice.

Simulation can be defined as the methods and procedures to define models of a system of interest and execute it
to get raw data [@osais_computer_2017]. In normal simulation studies, we are not interested in the raw data by itself, but use it to calculate measures of interest regarding the system's performance. For instance, in the example
shown in @sec-introduction, we saw that measures of interest include the average time that a customer 
has to way in the checkout queue. We sometimes also call these raw data *synthetic data*, since this is 
not the actual data that we would collect in the physical world. Synthetic data has by itself sparked 
interest in recent years due to its potential to enhance how we train and validate machine learning models, 
especially regarding data privacy and robustness, or when training data is expensive or scarce
[@jordon_synthetic_2022; @van_breugel_synthetic_2023].

In the rest of this chapter, we will introduce the basic principles and notions needed to understand
how simulation works. We start with a gentle reminder of random numbers and distributions, and introduce 
standard methods of random number generation. We then move on into stochastic processes and how
discrete-event simulation works. After that, we present common statistical techniques to deal with the
output data of simulations and conclude the chapter with considerations about verification and validation 
of simulation studies.

## Dealing with Random Numbers

We refer to *random numbers* as realizations of random variables that follow probability distributions. The 
following elements completely determine the statistical behaviour of randon numbers:

- Their **type**: discrete or continuous?
- The form of their **probability distribution**: binomial, normal, exponential, Poisson, etc.
- The **joint** or **conditional** distributions associated with the phenomenon at hand.
- The specific **parameters** used for each probability distribution.

In this book, we will mainly deal with parametric probability distributions, although everything applies 
to non-parametric distributions as well. We will hint at specific differences when appropriate.

### Pseudorandom Number Generators

In general, any procedure to generate random numbers is called a *pseudorandom number generator* (PRNB).
A PRNB can be defined as a deterministic algorithm that, given an initial seed, produces a long sequence of numbers that mimic the statistical properties of truly random samples. Although the sequence is fully determined by the seed (so it is not truly random, hence *pseudorandom*), a good PRNG yields values that are uniformly distributed, have minimal serial correlation, and pass standard statistical tests. Important PRNG properties include period length, equidistribution, independence, speed, and reproducibility (the same seed reproduces the same sequence). For simulation work we typically prefer generators with very long periods and strong statistical quality while cryptographic applications require cryptographically secure PRNGs. PRNGs are used to produce uniform variates that are then transformed into other distributions via methods such as inverse transform sampling, acceptance–rejection, or composition.

Let's explore the properties of a specific PRNG, the Linear Congruential Generator (LCG) using the following Python code.

```{python}
import numpy as np
from scipy.stats import chisquare
from collections import defaultdict


class LCG:
    """
    X(n+1) = (a * X(n) + c) mod m
    """
    def __init__(self, seed, a, c, m):
        self._state = seed
        self.a = a
        self.c = c
        self.m = m
        self.seed = seed

    def next_int(self):
        """Generates the next pseudo-random integer 
        in the sequence."""
        self._state = (self.a * self._state + self.c) % self.m
        return self._state

    def generate(self, size):
        """Generates a sequence of integers and 
        normalizes them to [0, 1)."""
        sequence_int = []
        sequence_float = []
        # Reset state to seed for sequence generation
        self._state = self.seed 
        
        for _ in range(size):
            next_val = self.next_int()
            sequence_int.append(next_val)
            # Normalize to a float in [0, 1) by dividing by the modulus
            sequence_float.append(next_val / self.m)
            
        return np.array(sequence_int), np.array(sequence_float)

```

The LCG is one of the oldest and best known PRNG which are used to date. As can be seen in 
the code, it uses three integer parameters $a$, $c$ and the modulo $m$ and computes the 
next random number using the recurrence:

$$
X_{n+1} = (a X_n + c) \operatorname{mod} m
$$ {#eq-1}

Starting at $n=0$, we initialize $X_0$ to the random seed provided.

We can now use the generator as follows:

```{python}
# LCG Parameters (a 'poor' LCG to highlight the deterministic nature)
# A small modulus (m) leads to a short period and visible patterns.
SEED = 42
A = 65  # Multiplier
C = 1   # Increment
M = 2**10  # Modulus (1024) - A small M is used for demonstration purposes
SEQUENCE_SIZE = 1000

# 1. Initialize and Generate Sequence
prng = LCG(SEED, A, C, M)
int_sequence, float_sequence = prng.generate(SEQUENCE_SIZE)
int_sequence[:10]
```

We have now generated 1000 random numbers using LCG (only first 10 are shown). But how can we ensure 
if this PRNM works well in practice? We will look now at the **period length**, how to check for **uniformity**
and how to assert if there is **serial correlation**.

**Period length**

The period length assesses the number of values generated before the sequence of states returns to the first 
value (the starting state) for the first time. Note that in general the longer, the better. Note that 
in this case, the maximum possible period is $m$, the modulo of the generator. We can calculate this with a simple Python function as follows:

```{python}
def calculate_period(lcg_generator):
    """
    Calculates the period (cycle length) of the LCG.
    The period is the number of values generated before the sequence repeats.
    """
    initial_state = lcg_generator.seed
    current_state = initial_state
    
    # Check for the next state immediately after the seed to start the loop
    current_state = (lcg_generator.a * current_state + lcg_generator.c) % lcg_generator.m
    period = 1
    
    # Loop until the state returns to the initial seed
    while current_state != initial_state:
        current_state = (lcg_generator.a * current_state + lcg_generator.c) % lcg_generator.m
        period += 1
        
        # Safety break for potentially infinite loops in case of a non-standard LCG
        if period > lcg_generator.m:
           return f"Period is greater than modulus m ({lcg_generator.m}). Check parameters."
            
    return period

period = calculate_period(prng)
period
```

So in this case, our generator reaches the maximum period (1024), which is the best we can do.

**Tests for uniformity**

We want the generated random numbers to be uniformly generated (we will see later how generate numbers with 
different distributions started with uniformly generated random numbers). For this, we use the $\chi^2$ test
for uniformity:

- Null Hypothesis ($H_0$): The generated numbers are uniformly distributed.
- Alternative Hypothesis ($H_1$): The generated numbers are not uniformly distributed.

The main idea of this test is to divide the generated numbers in intervals, and check whether those
intervals contain roughly the same number of generated values (e.g. a flat histogram). Like in the classical 
$\chi^2$ test, we calculate the expected $E_i$ and the observed $O_i$ frequencies for each range and calculate the $\chi^2$ statistic as usual:

$$
\chi^2=\sum_{i=1}^k\frac{(O_i-E_i)^2}{E_i}
$$

We can use the following Python function:

```{python}
def chi_squared_uniformity_test(data_float, num_bins=10):
    """
    Statistical Test: Chi-Squared Goodness-of-Fit Test for Uniformity.
    """
    N = len(data_float)
    
    # 1. Bin the data to get observed frequencies
    # The bins are equal-sized intervals in [0, 1).
    observed_frequencies, _ = np.histogram(data_float, bins=num_bins, range=(0, 1))
    
    # 2. Calculate expected frequencies for a perfectly uniform distribution
    expected_frequency = N / num_bins
    expected_frequencies = np.full(num_bins, expected_frequency)
    
    # 3. Perform the Chi-Squared test
    # The 'chisquare' function compares observed and expected frequencies.
    # A small p-value (e.g., < 0.05) leads to rejection of H0, meaning non-uniformity.
    chi2_stat, p_value = chisquare(f_obs=observed_frequencies, f_exp=expected_frequencies)
    
    return chi2_stat, p_value, num_bins

chi2_stat, p_value_uniformity, num_bins = chi_squared_uniformity_test(float_sequence)
print(f'Chi2 statistic: {chi2_stat}, p-value: {p_value_uniformity}, number of bins: {num_bins}')
```

In this case, the p-value is much higher than $\alpha=0.05$ and we **cannot** reject $H_0$, so 
the numbers appear to be uniformly random.