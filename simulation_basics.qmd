

# Simulation basics {#sec-simulation-basics}

## What is Simulation?

In science and engineering, it is of paramount importance to develop reliable quantitative models
 that capture the essential behavior of real systems. Simulation provides a 
 controlled, repeatable, and cost‑effective way to

- predict system behavior under varied conditions,
- explore "what‑if" scenarios and design alternatives,
- quantify uncertainty and sensitivity to inputs,
- validate hypotheses when experiments are impractical or expensive,
- and support optimization and decision making.

A simulation study typically involves the following steps:

1. Construct a mathematical or computational model.
2. Specify inputs and assumptions. 
3. Run experiments (often many replications with different parameters).
4. Analyze outputs and comparing them with data or theoretical expectations. 

Proper validation and uncertainty quantification are critical to ensure that simulation results are trustworthy and useful for engineering practice.

Simulation can be defined as the methods and procedures to define models of a system of interest and execute it
to get raw data [@osais_computer_2017]. In normal simulation studies, we are not interested in the raw data by itself, but use it to calculate measures of interest regarding the system's performance. For instance, in the example
shown in @sec-introduction, we saw that measures of interest include the average time that a customer 
has to way in the checkout queue. We sometimes also call these raw data *synthetic data*, since this is 
not the actual data that we would collect in the physical world. Synthetic data has by itself sparked 
interest in recent years due to its potential to enhance how we train and validate machine learning models, 
especially regarding data privacy and robustness, or when training data is expensive or scarce
[@jordon_synthetic_2022; @van_breugel_synthetic_2023].

In the rest of this chapter, we will introduce the basic principles and notions needed to understand
how simulation works. We start with a gentle reminder of random numbers and distributions, and introduce 
standard methods of random number generation. We then move on into stochastic processes and how
discrete-event simulation works. After that, we present common statistical techniques to deal with the
output data of simulations and conclude the chapter with considerations about verification and validation 
of simulation studies.

## Dealing with Random Numbers

We refer to *random numbers* as realizations of random variables that follow probability distributions. The 
following elements completely determine the statistical behaviour of randon numbers:

- Their **type**: discrete or continuous?
- The form of their **probability distribution**: binomial, normal, exponential, Poisson, etc.
- The **joint** or **conditional** distributions associated with the phenomenon at hand.
- The specific **parameters** used for each probability distribution.

In this book, we will mainly deal with parametric probability distributions, although everything applies 
to non-parametric distributions as well. We will hint at specific differences when appropriate.

### Pseudorandom Number Generators

In general, any procedure to generate random numbers is called a *pseudorandom number generator* (PRNB).
A PRNB can be defined as a deterministic algorithm that, given an initial seed, produces a long sequence of numbers that mimic the statistical properties of truly random samples. Although the sequence is fully determined by the seed (so it is not truly random, hence *pseudorandom*), a good PRNG yields values that are uniformly distributed, have minimal serial correlation, and pass standard statistical tests. Important PRNG properties include period length, equidistribution, independence, speed, and reproducibility (the same seed reproduces the same sequence). For simulation work we typically prefer generators with very long periods and strong statistical quality while cryptographic applications require cryptographically secure PRNGs. PRNGs are used to produce uniform variates that are then transformed into other distributions via methods such as inverse transform sampling, acceptance–rejection, or composition.

Let's explore the properties of a specific PRNG, the Linear Congruential Generator (LCG) using the following Python code.

```{python}
import numpy as np
from scipy.stats import chisquare
from collections import defaultdict


class LCG:
    """
    X(n+1) = (a * X(n) + c) mod m
    """
    def __init__(self, seed, a, c, m):
        self._state = seed
        self.a = a
        self.c = c
        self.m = m
        self.seed = seed

    def next_int(self):
        """Generates the next pseudo-random integer 
        in the sequence."""
        self._state = (self.a * self._state + self.c) % self.m
        return self._state

    def generate(self, size):
        """Generates a sequence of integers and 
        normalizes them to [0, 1)."""
        sequence_int = []
        sequence_float = []
        # Reset state to seed for sequence generation
        self._state = self.seed 
        
        for _ in range(size):
            next_val = self.next_int()
            sequence_int.append(next_val)
            # Normalize to a float in [0, 1) by dividing by the modulus
            sequence_float.append(next_val / self.m)
            
        return np.array(sequence_int), np.array(sequence_float)

```

The LCG is one of the oldest and best known PRNG which are used to date. As can be seen in 
the code, it uses three integer parameters $a$, $c$ and the modulo $m$ and computes the 
next random number using the recurrence:

$$
X_{n+1} = (a X_n + c) \operatorname{mod} m
$$ {#eq-1}

Starting at $n=0$, we initialize $X_0$ to the random seed provided.

We can now use the generator as follows:

```{python}
# LCG Parameters (a 'poor' LCG to highlight the deterministic nature)
# A small modulus (m) leads to a short period and visible patterns.
SEED = 42
A = 65  # Multiplier
C = 1   # Increment
M = 2**10  # Modulus (1024) - A small M is used for demonstration purposes
SEQUENCE_SIZE = 100000

# 1. Initialize and Generate Sequence
prng = LCG(SEED, A, C, M)
int_sequence, float_sequence = prng.generate(SEQUENCE_SIZE)
int_sequence[:10]
```

We have now generated 100000 random numbers using LCG (only first 10 are shown). But how can we ensure 
if this PRNM works well in practice? We will look now at the **period length**, how to check for **uniformity**
and how to assert if there is **serial correlation**.

**Period length**

The period length assesses the number of values generated before the sequence of states returns to the first 
value (the starting state) for the first time. Note that in general the longer, the better. Note that 
in this case, the maximum possible period is $m$, the modulo of the generator. We can calculate this with a simple Python function as follows:

```{python}
def calculate_period(lcg_generator):
    """
    Calculates the period (cycle length) of the LCG.
    The period is the number of values generated before the sequence repeats.
    """
    initial_state = lcg_generator.seed
    current_state = initial_state
    
    # Check for the next state immediately after the seed to start the loop
    current_state = (lcg_generator.a * current_state + lcg_generator.c) % lcg_generator.m
    period = 1
    
    # Loop until the state returns to the initial seed
    while current_state != initial_state:
        current_state = (lcg_generator.a * current_state + lcg_generator.c) % lcg_generator.m
        period += 1
        
        # Safety break for potentially infinite loops in case of a non-standard LCG
        if period > lcg_generator.m:
           return f"Period is greater than modulus m ({lcg_generator.m}). Check parameters."
            
    return period

period = calculate_period(prng)
period
```

So in this case, our generator reaches the maximum period (1024), which is the best we can do.

**Tests for uniformity**

We want the generated random numbers to be uniformly generated (we will see later how generate numbers with 
different distributions started with uniformly generated random numbers). For this, we use the $\chi^2$ test
for uniformity:

- Null Hypothesis ($H_0$): The generated numbers are uniformly distributed.
- Alternative Hypothesis ($H_1$): The generated numbers are not uniformly distributed.

The main idea of this test is to divide the generated numbers in intervals, and check whether those
intervals contain roughly the same number of generated values (e.g. a flat histogram). Like in the classical 
$\chi^2$ test, we calculate the expected $E_i$ and the observed $O_i$ frequencies for each range and calculate the $\chi^2$ statistic as usual:

$$
\chi^2=\sum_{i=1}^k\frac{(O_i-E_i)^2}{E_i}
$$

We can use the following Python function:

```{python}
def chi_squared_uniformity_test(data_float, num_bins=10):
    """
    Statistical Test: Chi-Squared Goodness-of-Fit Test for Uniformity.
    """
    N = len(data_float)
    
    # 1. Bin the data to get observed frequencies
    # The bins are equal-sized intervals in [0, 1).
    observed_frequencies, _ = np.histogram(data_float, bins=num_bins, range=(0, 1))
    
    # 2. Calculate expected frequencies for a perfectly uniform distribution
    expected_frequency = N / num_bins
    expected_frequencies = np.full(num_bins, expected_frequency)
    
    # 3. Perform the Chi-Squared test
    # The 'chisquare' function compares observed and expected frequencies.
    # A small p-value (e.g., < 0.05) leads to rejection of H0, meaning non-uniformity.
    chi2_stat, p_value = chisquare(f_obs=observed_frequencies, f_exp=expected_frequencies)
    
    return chi2_stat, p_value, num_bins

chi2_stat, p_value_uniformity, num_bins = chi_squared_uniformity_test(float_sequence)
print(f'Chi2 statistic: {chi2_stat}, p-value: {p_value_uniformity}, number of bins: {num_bins}')
```

In this case, the p-value is much higher than $\alpha=0.05$ and we **cannot** reject $H_0$, so 
the numbers appear to be uniformly random.

**Serial correlation**

The next possible measure to check is the serial correlation between the numbers generated. 
The **Serial Correlation Check**, also known as **Autocorrelation at Lag 1**, is a diagnostic measure used to characterize 
and detect a fundamental weakness in simple Pseudorandom Number Generators (PRNGs), such as the Linear Congruential Generator (LCG).
The main idea is that the correlation between immediately adjacent numbers (hence lag 1) should be zero.

To calculate this, we form two sequences: the generated numbers and the same sequence moved by one place:

$$
\begin{aligned}
S_n & = \{X_1,X_2,X_3,\dots,X_{n-1}\} \\
S_{n+1} & = \{X_2,X_3,X_4,\dots,X_n\}
\end{aligned}
$$

And now we calculate the Pearson correlation coefficient between $S_1$ and $S_2$.

$$
r=\frac{\sum (S_1-\bar{S_1})(S_2-\bar{S_2})}{\sqrt{\sum (S_1-\bar{S_1})^2(S_2-\bar{S_2})^2}}
$$

Our goal is that $r$ is as close to zero as possible (note that $r\in[-1,1]$). Let's use 
the following code:

```{python}
def serial_correlation_check(data_float):
    """
    Characterization: Autocorrelation (Serial Correlation) Check.
    """
    # X_n: all values except the last one
    X_n = data_float[:-1]
    # X_{n+1}: all values except the first one
    X_n_plus_1 = data_float[1:]
    
    # Calculate the Pearson correlation coefficient (r)
    # The result is an array, we take the correlation between the two sequences (index 0, 1)
    correlation_matrix = np.corrcoef(X_n, X_n_plus_1)
    lag_1_correlation = correlation_matrix[0, 1]
    
    return lag_1_correlation

lag_1_correlation = serial_correlation_check(float_sequence)
print(f'The lag 1 correlation coefficient is {lag_1_correlation}')
```
While the value is low, it's not as close to zero as it should, which is a known 
weakness of the LCG (the generated numbers tend to fall onto a number of parallel 
hyperplanes). This is the reason why PRNM like the LCG are not normally used in practice.
The de-facto standard for pseudorandom number generation in practice is the algorithm
known as the **Mersenne Twister**. This is the default generator used in Python or MATLAB, 
and the preferred one for simulation purposes (but *not* for cryptographic purposes). The 
basic idea is to use a highly non-linear twisted generalized feedback shift register. 
Apart from being much faster than LCG, it passes the serial correlation chek with flying colors:

```{python}
import random

random.seed(SEED)

# Generate a sequence of random floats in the range [0.0, 1.0)
float_sequence_mt = np.array([random.uniform(0, 1) for _ in range(SEQUENCE_SIZE)])

# Serial Correlation Check
lag_1_correlation_mt = serial_correlation_check(float_sequence_mt)
print(f'The lag 1 correlation coefficient is {lag_1_correlation_mt}')

```
which is an order of magnitude better than the LCG.

## Sampling Methods

We have now a method for generating *uniformly distributed* random numbers. But what about other 
widely used distributions, like normal, exponential, Poisson, etc? In this section, we will review 
three popular methods for this purpose: the **inversion method**, the **rejection sampling** method, the **Box-Muller transform** and 
the **mixture method**. For all three methods, the general problem is as follows: we start 
with a random variable $U \sim \text{ Uniform}(0,1)$. We want to convert $U$ into $X \sim f(x)$, 
where $f$ is the target PDF of $X$.



**Inversion method**

Suppose that we know the CDF of the target distribution $F(x)=P(X\le x)$, and assume that 
we can invert it to $F^{-1}(u)$. With this function, we can simply obtain $X$ by

$$
X=F^{-1}(U)
$$ {#eq-inv}

For instance, imagine our target distribution is the exponential, with density function
$f(x)=\lambda e^{-\lambda x}$. Elementary calculus tells us that $F(x)=1-e^{\lambda x}$. 
It can be shown that the inverse is

$$
F^{-1}(u)=-\frac{1}{\lambda}\ln(1-u)
$$ {#eq-example-inv}

Since $U'=1-U$ is also uniform in $[0,1]$, we can simply write $X=-\frac{1}{\lambda}\ln(U')$.

**Rejection-sampling method**

But what if our CDF is not easily invertible, or worse, we don't have any analytical expression
for it? Suppose that, although we don't have $f$, we have a proposal distribution $g(x)$ so that 
it "envelopes" the target distribution in the sense that there is a constant $c$ so 
that $f(x)\le c g(x)$ (i.e., we **do** know the PDF). In this case, we can do the following:

1. Sample $x$ from the proposal distribution $g(x)$.
2. Sample a uniform $U(0,1)$ random variable $u$.
3. If $u < \frac{f(x)}{c g(x)}$, the candidate number $x$ is accepted since it follows $f(x)$.
4. Otherwise, we repeat the procedure until we get a candidate accepted.

The trick now is to take a *bounded* uniform distribution as $g$ that contains our target 
distribution $f$. Once we have this, we can generate samples from virtually any 
probability distribution without requiring its CDF or inverse.

**Box-Muller transform**

The next method is specialized towards generating values for the **normal distribution**, and 
is widely used in practice. We start by generating two uniform random numbers $u_1, u_2 \sim U(-1,1)$.

* First, we calculte the sum of their squares $S=u_1^2+u_2^2$.
* If $S\ge 1$ or $S=0$, we reject both and return to the first step.
* Otherwise, we calculate $C=\sqrt{\frac{-2\ln(S)}{S}}$.
* We output two normally distributed random numbers as $z_1=u_1 C$ and $z_2 = u_2 C$.

The random numbers generated follow a standard normal distribution $N(0,1)$. For an arbitrary
normal distribution $N(\mu,\sigma^2)$ we just scale using the standard transformation $X=\mu+\sigma Z$.

**Mixture method**

In the case that the target distribution can be expressed as a mixture of $k$ different PDFs

$$
f(x)=\sum_{i=1}^k p_i f_i(x)\text{, with }p_i\ge 0\text{, and }\sum_{i=1}^k p_i = 1
$$ {#eq-mixtures}

Then we can use the following methods to sample from $f(x)$:

* Choose randomly an index $i\in I$ from the set of indices $I=\{1,2,\dots,k\}$. This is done by
generating a random number $u \sim U(0,1)$ and choosing the least index $j$ so that 
$\sum_{i=1}^j p_i < u$.
* Generate a random variable $x$ from $f_i(x)$ by using any of the aforementioned methods.

This is a suitable method, for instance, to generate random numbers that follow a 
**Gaussian Mixture Model (GMM)**. In this case, we just sample an index and generate a 
random number according to the Box-Muller method, scaling accordingly if necessary.

## Stochastic Processes

Now that we know how to generate random numbers and sample from different distributions, we need
to understand how they interact over time in a simulation study. This is the realm of *stochastic processes*.

A stochastic process $\{X(t), t \in T\}$ is a collection of random variables indexed by time. The set $T$ can be:

- **Discrete**: $T = \{0, 1, 2, ...\}$ or $T = \{t_0, t_1, t_2, ...\}$
- **Continuous**: $T = [0, \infty)$ or $T = [a, b]$

For each fixed time $t$, $X(t)$ is a random variable. For each sample path (realization) of the process,
$X(t)$ is a function of time. The nature of the state space (the set of possible values of $X(t)$) leads
to different classifications:

- **Discrete state space**: The process can only take certain values (e.g., number of customers in a queue)
- **Continuous state space**: The process can take any value in an interval (e.g., temperature in a reactor)

Understanding stochastic processes is crucial for simulation because they model how random events unfold
over time, which is exactly what we need to simulate complex systems with uncertainty.

**Stationary and non-stationary processes**

A stochastic process $\{X(t)\}$ is said to be **stationary** if its statistical properties do not change over time. More formally:

- The mean function is constant: $E[X(t)] = \mu$ for all $t\in T$.
- The variance function is constant: $Var[X(t)] = \sigma^2$ for all $t\in T$.
- The autocovariance function depends only on the time difference: $Cov[X(t), X(s)] = C(|t-s|)$.

In contrast, a **non-stationary** process has statistical properties that vary with time. For example:

- A random walk is non-stationary because its variance increases with time.
- A seasonal time series with repeating patterns is non-stationary.
- A process with a trend component is non-stationary.

Stationarity is an important property in simulation because it allows us to:

- Make meaningful predictions about future behavior.
- Estimate parameters from historical data.
- Apply many statistical techniques that assume stationarity.

In the following we will see examples of different stochastic processes and how to simulate them efficiently.

### Bernoulli and Binomial Processes

We call a Bernoulli process a sequence of independent trials with two possible outcomes ("success/failure").
The classical example is flipping a coin independently for $n$ times.

![A typical Bernoulli process.](figures/chapter_2/fig_bernoulli){#fig-bernoulli}

We formally define a **Bernoulli process** as follows:

- Each variable takes values from the set $\{0,1\}$. In our example, the value 0 could stand for 
heads, and 1 for tails.
- The probability of sucess (per convention, $P(success)=P(X(t)=1)$) is the same for every trial.
- The outcome for a given trial is independent of any other trial. So in our case, each coin flip 
is independent of all the others.

$$ 
P(X(t)=1)=p\text{ and }P(X(t)=0)=1-p
$$

The **Bernoulli counting process** $\{N(t)\}$ is just the sum of the outcomes of the first $t$ trials.

$$
N(t)=\sum_{i=1}^t X(i)
$$

where $X(i)$ is a Bernoulli random variable with parameter $p$. So $N(t)$ counts the number of 
successes occurred in trials 1 to $t$. Some properties of the counting process are:

- Each variable $N(t)$ takes values in the set $\{0,1,2,\dots,t\}$.
- The **increment** of this process $N(t)-N(t-1)$ is equals to the result of the $t$-th trial, $X(t)$. 
- $N(t)$ follows a **binomial distribution**:

$$
P(N(t)=k)=\binom{t}{k}p^k(1-p)^{z-k}
$$ {#eq-binomial}

Let's see how to simulate a Bernoulli process:

```{python}
import random

def simulate_bernoulli_process(num_trials, success_probability):
    """
    Simulates a Bernoulli process for a given number of trials and success probability.
    """
    if not 0 <= success_probability <= 1:
        raise ValueError("Success probability must be between 0 and 1.")

    bernoulli_outcomes = [
        1 if random.random() < success_probability else 0
        for _ in range(num_trials)
    ]
    
    return bernoulli_outcomes
```

Here we are just using Python's default random number generator (the Mersenne Twister) to 
check if the generated number is below or above the sucess probability, in the former 
case we count a success, otherwise a failure. We can now run the simulation for both
Bernoulli and Bernoulli counting processes and visualize the results.

```{python}
import matplotlib.pyplot as plt
import numpy as np
P_SUCCESS = 0.3  # Probability of success (p)
NUM_TRIALS = 50  # Total number of trials
random.seed(42)  # Set seed for reproducibility
outcomes = simulate_bernoulli_process(NUM_TRIALS, P_SUCCESS)
counting_process = np.cumsum(outcomes)
```

In this case, the Bernoulli counting process is just the cumulative sum of the generated 
Bernoulli process.

```{python}
#| fig-cap: "Plots of Bernoulli and counting processes"
trials = np.arange(1, NUM_TRIALS + 1)

# plt.figure(figsize=(12, 6))

# Subplot 1: The Bernoulli Process (Outcomes)
plt.subplot(2, 1, 1)
plt.step(trials, outcomes, where='post', color='blue', linewidth=2)
plt.yticks([0, 1], ['Failure (0)', 'Success (1)'])
plt.title('Bernoulli Process (Sequence of Trials)')
plt.xlabel('Trial Number (i)')
plt.ylabel('Outcome ($X_i$)')
plt.grid(axis='y', linestyle='--')
plt.ylim(-0.1, 1.1)

# Subplot 2: The Bernoulli Counting Process (Cumulative Sum)
plt.subplot(2, 1, 2)
plt.step(trials, counting_process, where='post', color='red', linewidth=2)
plt.title('Bernoulli Counting Process (Cumulative Successes)')
plt.xlabel('Trial Number (t)')
plt.ylabel('Count ($N(t)$)')
plt.axhline(NUM_TRIALS * P_SUCCESS, color='green', linestyle=':', label='Expected Count')
plt.legend()
plt.grid(True, linestyle='--')

plt.tight_layout()
plt.show()
```

For this plot, we have generated $T=50$ trials with a success probability of $p=0.3$. The Bernoulli process
outcomes are depicted in the upper part of the figure. In the lower part, the evolution of the 
counting process $\{N(t)\}$ is shown. The expected count is just calculated by multiplying the number 
of trials with the success probability, which results in 15 as the expected number of successes.

### Poisson Processes

We now switch from *continuous* to *discrete* time. The **Poisson process** is a canonical 
continous time counting process. It tracks 
the number of occurrences of a given event up to a certain time $t$. Formally, let $\lambda > 0$ be the 
rate of occurrences of the event in question. We define the Poisson process $\{N(t),t\ge 0\}$ as follows:

- As initial condition, we set $N(0)=0$.
- If we consider two non-overlapping intervals $[s,t]\cap [u,v]=\emptyset$, then the increments on those
intervals are **independent** of each other:

$$
N(t)-N(s)\text{ is independent of }N(v)-N(u), [s,t]\cap [u,v]=\emptyset
$$ {#eq-poisson-intervals}

- At any interval of length $\tau=t-s$, the increments follow a Poisson distribution with 
parameter $\lambda\tau$.

$$
N(t)-N(s) \sim \operatorname{Poisson}(\lambda\tau)
$$ {#eq-poisson-increment}

$$ 
P(N(t)-N(s)=k)=e^{-\lambda\tau}\frac{(\lambda\tau)^k}{k!}
$$ {#eq-poisson-probability}

The Poisson process is one of the most widely used stochastic models in science, engineering, and finance due to its ability to accurately model random, independent, and rare events occurring over time or space. Its practical relevance stems directly from the three conditions of its definition (independent, stationary increments, and the resulting Poisson distribution). For instance, the arrival of customer calls in a
call center can be accurately modeled using Poisson processes. In engineernig, the number of defects
found per area or volume (e.g. on a silicon wafer) can be also modeled this way (in this case, the rate 
is *spatial* rather than *temporal*, but the concept remains the same). In general,
customer arrivals (at a hospital, a bank, a supermarket) can be also modeled using a Poisson process.

One of the main properties that makes the Poisson process useful in practice is the fact that is 
*memoryless*: the time until the next event is independent of how long it has been since the 
previous event, which aligns well with phenomena found in practice. Another advantage is that since 
the process is based in the Poisson (for the number of events per time interval) and the 
exponential distributions (for the time between events), the process is mathematically well 
tractable and closed-form solutions exists for many situations of interest.

The next code snippet demonstrates how to simulate a Poisson process.

```{python}
import numpy as np
import matplotlib.pyplot as plt
import random

def simulate_poisson_process(rate_lambda, max_time):
    if rate_lambda <= 0 or max_time <= 0:
        raise ValueError("Rate and max_time must be positive.")

    # Set up the event simulation
    current_time = 0.0
    event_times = [0.0]  # Start at time 0 with the first "event"

    while current_time < max_time:
        # 1. Generate the next inter-arrival time
        time_until_next_event = random.expovariate(rate_lambda)
        
        # 2. Update the cumulative time
        current_time += time_until_next_event
        
        # 3. Record the event time if it's within the simulation duration
        if current_time < max_time:
            event_times.append(current_time)

    # Calculate the event count at each recorded time
    num_events = len(event_times) - 1 # N(0)=0 is the first entry
    num_events_at_t = list(range(num_events + 1))
    
    return event_times, num_events_at_t
```

In this code we exploit the fact that the interarrival times are exponentially distributed, 
so we sample a time and count it as a new event. We can now use the code 
to simulate the process:

```{python}
RATE_LAMBDA = 2.0  # Average rate of 2 events per unit of time
MAX_TIME = 10.0    # Simulate over 10 time units

random.seed(42) # Set seed for reproducibility
event_times, cumulative_counts = simulate_poisson_process(RATE_LAMBDA, MAX_TIME)
event_times[:5]
```

Like in the previous case, we can perform some visualizations to help understand 
the evolution of the process.

```{python}
# Plot the step function of the counting process N(t)
# We use drawstyle='steps-post' to create the classic step-function look
plt.plot(event_times, cumulative_counts, drawstyle='steps-post', color='darkorange', linewidth=2, marker='o', markersize=4)

# Plot the expected count line
plt.axhline(RATE_LAMBDA * MAX_TIME, color='gray', linestyle='--', alpha=0.6, label='Expected Final Count')

plt.title(f'Poisson Counting Process $N(t)$ with $lambda = {RATE_LAMBDA}$')
plt.xlabel('Time (t)')
plt.ylabel('Number of Events ($N(t)$)')
plt.xlim(0, MAX_TIME)
plt.ylim(bottom=0)
plt.grid(True, linestyle='--')
plt.legend()
plt.show()
```