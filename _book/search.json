[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Simulation and Optimization: A Model-Driven Approach",
    "section": "",
    "text": "Welcome\nThis is the website for the Simulation and Optimization book that will teach you the basics of simulation approaches and optimization techniques in the context of modern AI systems. The source code of the book and the examples are provided as open-source and free to use Creative Commons Attribution-NonCommercial-NoDerivs 4.0 license.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Preface",
    "section": "",
    "text": "This book was created as companion material for a semester graduate course on simulation and optimization. It is the author’s opinion that in an age of rapid advances in the field of artificial intelligence, it is of utmost importance to focus not only on machine learning, but to study in detail the techniques that make current advances in AI possible. From those, the areas of simulation and optimization have the highest potential to reveal how intrincate current AI is intertwined with other areas of mathematics, statistics and computer science.\nSimulation techniques are widely used in many scientific disciplines, ranging from climate models, epidemiology, and engineering to finance and logistics. These methods allow researchers and practitioners to analyze complex systems, evaluate scenarios, and make informed decisions when analytical solutions are infeasible or unavailable. Throughout this book, we will explore foundational concepts and practical approaches to simulation and optimization, providing both theoretical background and hands-on examples. In the context of AI, simulation approaches can be used to produce synthetic data for training in situations where these data are scarce, expensive, or simply impossible to collect. Another uses of simulation approaches include stress-testing algorithms, validating models under various hypothetical scenarios, and supporting decision-making in uncertain environments. By leveraging simulation, practitioners can gain insights into system behavior, identify potential risks, and optimize performance before deploying solutions in real-world settings.\nOptimization approaches lie at the core of how machine learning is used in modern AI systems. Foundational algorithms like stochastic gradient descent make it possible to find optimal parameters for machine learning models using training datasets composed of millions of data points. Additionaly, optimization algorithms are used for hyperparameter tuning and can be found at the heart of classical approaches like support vector machines and logistic regression. In this context, both classical and metaheuristic approaches play a pivotal role in finding optimal or near-optimal solutions which are used in the broader context of specific applications in practice.\nThroughout the book, we will assume that the reader has familiarity with linear algebra and calculus and possesses a good command of statistics and the basics of machine learning. Additionally, good background knowledge of the Python programming language is adviced for the practical part of this book.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Example 1: Simulating supermarket dynamics\nSimulation and optimization approaches are present in our everyday lives, albeit most of the time operating in a background plane. For example, when navigating with a GPS, the system simulates different routes and optimizes for the shortest or fastest path. Similarly, supply chains use optimization algorithms to minimize costs and maximize efficiency, while simulations help predict demand and manage inventory. These techniques are fundamental tools in decision-making processes across various industries, from transportation and logistics to finance and healthcare.\nBut what do simulation and optimization approaches have in common, apart from being complementary tools? The answer lies in the concept of a model. In the context of machine learning, we normally refer to a model as a mathematical or computational representation that captures the relationships between input data and output predictions. In simulation and optimization, a model similarly serves as an abstraction of a real-world system or process, allowing us to analyze, predict, and improve its behavior through experimentation and algorithmic techniques.\nIn the following, we will delve deeper into the concept of a model and how models are used in simulation and optimization contexts using some practical examples.\nImagine you are in your favourite grocery store waiting at the checkout queue. For simplicity, let’s assume there is only one open counter. When you arrive at the queue, there might be other customers already waiting, while the first customer at the queue is currently being served. Shortly after you, a new customer arrives, taking the next free spot right behind you. And then another customer arrives, and another one, and another one…\nLet’s try to break down how this system behaves and what are the most important interactions between the parts of the system. In general, we will distinguish between components, states, events, inputs and metrics.\nThe system could be represented by the following Python code as a minimal variant.\nThis code assumes that customers arrive at regular subsequent intervals after each arrival event. The parameter sim_end defines how long (how many steps) we want to simulate in this case. The function service_time returns the time the cashier needs for checking out customer cid. The next customer will arrive after a time given by the function next_interarrival, which can implement different stochastic behaviours.\nWe can represent this system graphically as shown in the following illustration:\nIn this figure, customers are denoted by \\(r_i\\), the amount of cashiers is \\(c\\) and the total number of customers in the supermarket at time \\(t\\) is denoted by \\(N(t)\\).\nNow let’s try to refine the dynamics of this system. We will now write some equations to describe the system’ dynamics according to the infinite waiting room \\(M/M/c\\) model. Let’s make the following assumptions:\nWhat would be now the traffic intensity per cashier? That is, what is the mean customer flow that each cashier experiences from their own point of view? Let’s call this number \\(\\rho\\) and calculate it as follows:\n\\[\n\\rho=\\frac{\\lambda}{c\\mu}\n\\tag{1.1}\\]\nIn words, if customers arrive at a rate of \\(\\lambda=10\\) customers/s and each cashier serves 2 customers/s (yes, it’s a fast supermarket). With 5 cashiers, that means that \\(\\rho=10/5\\times 2=1\\). This means that each cashier has quite a lot to do right now.\nWe are now interested in the probabilities of the states in this systems. In this case, we define a system by the number of customers currently present in the supermarket. So we can have \\(N=1\\) if there is currently 1 customer present, or any other number of customers (we assume the supermarket is so large, we can accomodate any number of them). Let’s denote these probabilities by \\(p_n=\\operatorname{Pr}\\{N=n\\}\\). We have:\n\\[\np_n=\\lim_{t\\rightarrow\\infty}\\int_{0}^t \\mathbb{1}_{\\{N(s)=n\\}} ds\n\\tag{1.2}\\]\nIntuitively, \\(p_n\\) represents the fraction of time where the supermarket has exactly \\(n\\) customers. As mentioned earlier, we will assume that arrivals do not depend of the current state \\(n\\), so we write \\(\\lambda_n=\\lambda\\) for all \\(n\\ge 0\\). However, note that the completion rates \\(\\mu_n\\) do indeed depend of the current state. To see this, imagine that there is only one customer in the supermarket (\\(n=1\\)). The completion rate is then \\(\\mu_1=\\mu\\) since the only one cashier is needed to perform checkout. However, if there are \\(n=2\\) customers in the supermarket, two cashiers can serve those two customers in parallel, increasing the completion rate to \\(\\mu_2 = 2\\mu\\). The same reasoning applies until \\(n=c\\), the total number of cashiers. In this case, \\(\\mu_c=c\\mu\\) and the next customer will have to wait in the queue. So we have:\n\\[\n\\begin{aligned}\n\\lambda_n & =\\lambda \\text{ for all } n\\ge 0 \\\\\n\\mu_n & =\\min(n,c)\\mu\n\\end{aligned}\n\\tag{1.3}\\]\nNow we are going to state our main modeling assumption. Consider how we transition between states. Specifically, we transition from state \\(n\\) to state \\(n+1\\) when a new customer enters the supermarket, and there were already \\(n\\) customers in it. Similarly, we transition from state \\(n\\) to state \\(n-1\\) when a customer leaves the supermarket (in this case, all customers are served by the cashiers, so there is no way you leave the supermarket without paying first). Remember that the rate of customers arriving at the supermarket is always \\(\\lambda\\), and the rate of customers being served (i.e. leaving) when at state \\(n\\) is \\(\\mu_n\\). In general, for each state we can define an incoming and an outgoing flow. This quantifies the transitions in resp. out of a given state.\nAs can be seen in the previous figure, transitions flow away from state \\(n\\) in two ways: first, to state \\(n-1\\) when a customer is served with a rate \\(\\mu_n p_n\\) and to state \\(n+1\\) when a new customer arrives with a rate \\(\\lambda p_n\\). Similarly, one can transition from the other states to state \\(n\\) either by having a customer served in state \\(n+1\\) with a rate \\(\\mu_{n+1}p_{n+1}\\) or when being in state \\(n-1\\) and a new customer arrives with a rate \\(\\lambda p_{n-1}\\). Our modeling assumption now is that, for each state \\(n\\), the flow outwards balances out with the flow inwards (global balance):\n\\[\n\\lambda p_{n-1} + \\mu_{n+1}p_{n+1} = \\lambda p_n + \\mu_n p_n\\text{,   for }n\\ge 1\n\\tag{1.4}\\]\nWe can also write that, in the long term, the rate of transitions from \\(n\\) to \\(n+1\\) equals the transitions from \\(n+1\\) to \\(n\\), which results in the more simple form (local balance)\n\\[\n\\lambda p_n = \\mu_{n+1}p_{n+1} \\text{,   for }n\\ge 1\n\\tag{1.5}\\]\nThis form follows from the global balance condition when only neighboring states are connected. Now, using this short form, we can provide closed-form expressions for different probabilities, using the following recursion (which directly follows from the above)\n\\[\np_{n+1}=\\frac{\\lambda}{\\mu_{n+1}}p_n\n\\tag{1.6}\\]\nFor instance, one can calculate that the probability of a customer having to wait (because all cashiers are busy at the moment) is\n\\[\nP_W=p_0\\frac{(\\lambda/\\mu)^c}{c!}\\frac{1}{1-\\rho}\n\\tag{1.7}\\]\nThis is also called the Erlang-C probability and we will delve deeper into the details in the coming chapters.\nWe can now write computer code that performs a step-by-step simulation of the system (i.e. in discrete time steps). This is specially useful if we are in a situation where there is no closed-form analytical solution, or the analytical solution is too complex to calculate. For instance, we can run the above simulator for a large number of steps (say \\(T=10^6\\)) and then calculate specific metrics like:\nWe will see examples of such simulators in the first part of the book.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#example-1-simulating-supermarket-dynamics",
    "href": "intro.html#example-1-simulating-supermarket-dynamics",
    "title": "1  Introduction",
    "section": "",
    "text": "Components: These are the entities that interact with each other. In our example, we have customers, cashiers and the queue itself.\nStates: The configurations of the system that represent valid combinations of specific properties of the components at a given moment of time. For instance, at each time the queue has a specific length: zero if it’s empty, one customer, two customers, etc. Additionally, the cashier can be busy or idle. We can also count the number of customers currently present in the supermarket which have not yet arrive at the checkout queue.\nEvents: The interactions themselves, like a new customer arriving at the queue, checkout start or checkout completion.\nInputs: Whatever information is fed into the system, e.g. arrival times, service times, etc. These inputs can contain statistical assumptions, like the distribution of arrival times.\nMetrics: How we evaluate the system as a whole in a given time step. For instance, what is the average waiting time? How much time are the cashiers busy? How is the queue length distributed?\n\n\nimport heapq, random\n\n# event = (time, type, customer_id)\nevent_list = []\nheapq.heappush(event_list, (first_arrival_time, 'arrival', 1))\n\nwhile event_list and time &lt; sim_end:\n    time, ev_type, cid = heapq.heappop(event_list)\n    if ev_type == 'arrival':\n        if any_cashier_free():\n            start_checkout(cid, time)\n            heapq.heappush(event_list, (time + service_time(cid), \n                'departure', cid))\n        else:\n            enqueue(cid, time)\n        heapq.heappush(event_list, (time + next_interarrival(), \n            'arrival', next_id()))\n    elif ev_type == 'departure':\n        finish_service(cid, time)\n        if queue_not_empty():\n            next_cid = dequeue()\n            start_service(next_cid, time)\n            heapq.heappush(event_list, (time + service_time(next_cid), \n                'departure', next_cid))\n\n\n\n\n\n\n\n\nFigure 1.1: An illustration of the supermarket dynamics.\n\n\n\n\n\n\nArrivals follow a Poisson distribution with mean \\(\\lambda\\) (arrivals per second), which for this case will be assumed to be stationary.\nThe service times are assumed to be exponentially distributed with mean \\(1/\\mu\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1.2: Transition dynamics between neighboring states\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMean queue length.\nMean waiting time.\nMean time in the system.\nTotal fraction of time that the cashiers were busy.\nOverall utilization.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#example-2-the-traveling-salesman",
    "href": "intro.html#example-2-the-traveling-salesman",
    "title": "1  Introduction",
    "section": "1.2 Example 2: The traveling salesman",
    "text": "1.2 Example 2: The traveling salesman\nLet’s not turn our attention to the other type of problems which are central to this book: optimization problems. Imagine you are a sales representative for a vaccum cleaner manufacturer. Your task is to visit potential customers in cities across your area and, at the end of the day, return to where you started your journey. As an environmental conscious employer and in order to save transport costs, your company introduces the restriction that each customer in the route has to be visited exactly once. So you need to think carefully before getting into your car and starting your route.\n\n\n\n\n\n\nFigure 1.3: The traveling salesman has to find a good and practical solution\n\n\n\nNow, the traveling salesman needs to consider two things:\n\nA method for constructing valid tours that start and end in the same location.\nA method for evaluating those tours so that we can quantitatively decide if a tour is better than another one.\n\nLet’s address each of these considerations in detail. Assume that \\(N=\\{1,2,\\dots,n\\}\\) is our set of possible locations. Let’s define the following variables:\n\\[\n\\begin{aligned}\nx_{ij}=\n\\begin{cases}\n1 & \\text{if the tour goes directly to location }i \\text{ to location }j \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\end{aligned}\n\\tag{1.8}\\]\nwhere \\(i,j\\in N\\). We call \\(x_{ij}\\) our decision variables. So if the traveling salesman specifies the value of each \\(x_{ij}\\), we have a candidate route to consider. However, not every assignment of the \\(x_{ij}\\) variables to \\(\\{0,1\\}\\) will make sense for the traveling salesman. For instance, imagine that we have in one assignment both \\(x_{23}=1\\) and \\(x_{43}=1\\). That would mean that location 3 is visited twice, once from location 2 and another time from location 4. That violates the requirement that each location is visited exactly once.\nTo model this situation, we need to introduce constraints. In optimization problems, constraints take usually the form of equalities or inequalities as functions of the decision variables. In our case, the requirement that each location is visited only once can be expressed by the following (linear) equalities:\n\\[\n\\begin{aligned}\n\\sum_{j=1}^n x_{ij} & = 1\\text{ for all }i\\in N \\\\\n\\sum_{i=1}^n x_{ij} & = 1\\text{ for all }j\\in N\n\\end{aligned}\n\\tag{1.9}\\]\nThe first equality means that, fixed a location \\(i\\), the sum of all outgoing edges is exactly one. Conversely, the second states that for a fixed location \\(j\\), the sum of all incoming edges is also exactly one. This ensures that each location is visited exactly once.\nWe need another technical condition to guarantee that the tour is a single one and not composed of multiple sub-tours. There should be no subset \\(S \\subset N\\) such that is self contained, the number of visited cities equals exactly its size. Mathematically:\n\\[\n\\sum_{i\\in N}\\sum_{j\\in N} x_{ij} \\le |S|-1\\text{ for all subsets }S\\subset N\n\\tag{1.10}\\]\nTo sum up, we now have modeled how valid tours should look like. If we find a tour \\(T=\\{x_{ij}\\}\\) that satisfies the constraints outlined before, we can be sure it is a valid tour.\nBut surely there are some tours that are better than others? This is where the second issue becomes important: we need an evaluation method to distinguish between good and bad solutions. In the optimization literature, we normally talk about objective functions. In our case, the traveling salesman would like the total distance to be minimized, meaning the sum of all distances between locations of the tour. Assume that \\(c_{ij}&gt;0\\) is the distance between location \\(i\\) and \\(j\\) (for consistency assume \\(c_{ii}=0\\)). Now we want to minimize the total distance traveled. For this we write:\n\\[\n\\min \\sum_{i\\in N}\\sum_{j\\in N} c_{ij}x_{ij}\n\\tag{1.11}\\]\nThat is, if the traveling salesman visits location \\(j\\) from \\(i\\), then \\(x_{ij}=1\\) and this activates the travel cost \\(c_{ij}\\) in the sum. Otherwise, \\(x_{ij}=0\\) and the cost does not count to the total sum, since that path is not traversed in the tour. Putting it all together, we have:\n\\[\n\\begin{aligned}\nT^* & =\\operatorname{argmin} \\sum_{i\\in N}\\sum_{j\\in N} c_{ij}x_{ij} \\\\\n\\text{s.t. } & \\sum_{j=1}^n x_{ij} = 1\\text{ for all }i\\in N \\\\\n& \\sum_{i=1}^n x_{ij} = 1\\text{ for all }j\\in N \\\\\n& \\sum_{i\\in N}\\sum_{j\\in N} x_{ij} \\le |S|-1\\text{ for all subsets }S\\subset N \\\\\n& x_{ij} \\in \\{0,1\\}\n\\end{aligned}\n\\tag{1.12}\\]\nWe call this set of expressions our optimization model. This will be the mathematical underpinning for all the methods and algorithms that we will use to find a solution to this problem. In the first line, we state our goal: to obtain a tour \\(T^*\\) that is optimal in the sense of minimizing the total cost (the expression \\(\\operatorname{argmin}\\) means “the argument that minimizes”, so find the \\(x_{ij}\\) that minimize the total cost function). The subsequent lines state the constraints that we listed before. In the last line we specify the domain of the decision variables, i.e. what are the possible values these variables can take.\nWe will see that, depending on the form of the optimization model we will be able to choose from a toolbox of algorithms capable to solve the problem at hand, either exactly (exact methods) or approximately (heuristic methods).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#structure-of-the-book",
    "href": "intro.html#structure-of-the-book",
    "title": "1  Introduction",
    "section": "1.3 Structure of the book",
    "text": "1.3 Structure of the book\nIn this first chapter, we have introduced the concept of a model and have applied it successfully to a simulation and an optimization problem. The rest of the book is structured in two parts: Part I will be dedicated to simulation approaches, including the \\(M/M/c\\) model we have seen in this chapter in Chapter 2. Monte Carlo methods are the main topic of Chapter 3. After that, Chapter 4 focuses on the handling of discrete events, while Chapter 5 concludes with considerations about agent-based modeling and simulation.\nPart II is dedicated to optimization problems. In Chapter 6 we introduce the mathematical basics of optimization. Chapter 7 is dedicated to exact optimization methods like the simplex method for linear programming. Approximate methods for complex optimization problems like metaheuristics and evolutionary algorithms are presented in Chapter 8. Finally, we review the importance of optimization methods for machine learning in Chapter 9.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#exercises",
    "href": "intro.html#exercises",
    "title": "1  Introduction",
    "section": "1.4 Exercises",
    "text": "1.4 Exercises\n\nProve that in the supermarket example the local balance condition follows from the global balance condition (Hint: use induction).\nWhat happens to the optimization model in presented in Equations 1.12 if we remove Equation 1.10? Find an example of a tour that is valid according to the model but invalid for the traveling salesman.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "simulation_basics.html",
    "href": "simulation_basics.html",
    "title": "2  Simulation basics",
    "section": "",
    "text": "2.1 What is Simulation?\nIn science and engineering, it is of paramount importance to develop reliable quantitative models that capture the essential behavior of real systems. Simulation provides a controlled, repeatable, and cost‑effective way to\nA simulation study typically involves the following steps:\nProper validation and uncertainty quantification are critical to ensure that simulation results are trustworthy and useful for engineering practice.\nSimulation can be defined as the methods and procedures to define models of a system of interest and execute it to get raw data (Osais 2017). In normal simulation studies, we are not interested in the raw data by itself, but use it to calculate measures of interest regarding the system’s performance. For instance, in the example shown in Chapter 1, we saw that measures of interest include the average time that a customer has to way in the checkout queue. We sometimes also call these raw data synthetic data, since this is not the actual data that we would collect in the physical world. Synthetic data has by itself sparked interest in recent years due to its potential to enhance how we train and validate machine learning models, especially regarding data privacy and robustness, or when training data is expensive or scarce (Jordon et al. 2022; Breugel, Qian, and Schaar 2023).\nIn the rest of this chapter, we will introduce the basic principles and notions needed to understand how simulation works. We start with a gentle reminder of random numbers and distributions, and introduce standard methods of random number generation. We then move on into stochastic processes and how discrete-event simulation works. After that, we present common statistical techniques to deal with the output data of simulations and conclude the chapter with considerations about verification and validation of simulation studies.",
    "crumbs": [
      "PART I: SIMULATION",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simulation basics</span>"
    ]
  },
  {
    "objectID": "simulation_basics.html#what-is-simulation",
    "href": "simulation_basics.html#what-is-simulation",
    "title": "2  Simulation basics",
    "section": "",
    "text": "predict system behavior under varied conditions,\nexplore “what‑if” scenarios and design alternatives,\nquantify uncertainty and sensitivity to inputs,\nvalidate hypotheses when experiments are impractical or expensive,\nand support optimization and decision making.\n\n\n\nConstruct a mathematical or computational model.\nSpecify inputs and assumptions.\nRun experiments (often many replications with different parameters).\nAnalyze outputs and comparing them with data or theoretical expectations.",
    "crumbs": [
      "PART I: SIMULATION",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simulation basics</span>"
    ]
  },
  {
    "objectID": "simulation_basics.html#dealing-with-random-numbers",
    "href": "simulation_basics.html#dealing-with-random-numbers",
    "title": "2  Simulation basics",
    "section": "2.2 Dealing with Random Numbers",
    "text": "2.2 Dealing with Random Numbers\nWe refer to random numbers as realizations of random variables that follow probability distributions. The following elements completely determine the statistical behaviour of randon numbers:\n\nTheir type: discrete or continuous?\nThe form of their probability distribution: binomial, normal, exponential, Poisson, etc.\nThe joint or conditional distributions associated with the phenomenon at hand.\nThe specific parameters used for each probability distribution.\n\nIn this book, we will mainly deal with parametric probability distributions, although everything applies to non-parametric distributions as well. We will hint at specific differences when appropriate.\n\n2.2.1 Pseudorandom Number Generators\nIn general, any procedure to generate random numbers is called a pseudorandom number generator (PRNB). A PRNB can be defined as a deterministic algorithm that, given an initial seed, produces a long sequence of numbers that mimic the statistical properties of truly random samples. Although the sequence is fully determined by the seed (so it is not truly random, hence pseudorandom), a good PRNG yields values that are uniformly distributed, have minimal serial correlation, and pass standard statistical tests. Important PRNG properties include period length, equidistribution, independence, speed, and reproducibility (the same seed reproduces the same sequence). For simulation work we typically prefer generators with very long periods and strong statistical quality while cryptographic applications require cryptographically secure PRNGs. PRNGs are used to produce uniform variates that are then transformed into other distributions via methods such as inverse transform sampling, acceptance–rejection, or composition.\nLet’s explore the properties of a specific PRNG, the Linear Congruential Generator (LCG) using the following Python code.\n\nimport numpy as np\nfrom scipy.stats import chisquare\nfrom collections import defaultdict\n\n\nclass LCG:\n    \"\"\"\n    X(n+1) = (a * X(n) + c) mod m\n    \"\"\"\n    def __init__(self, seed, a, c, m):\n        self._state = seed\n        self.a = a\n        self.c = c\n        self.m = m\n        self.seed = seed\n\n    def next_int(self):\n        \"\"\"Generates the next pseudo-random integer \n        in the sequence.\"\"\"\n        self._state = (self.a * self._state + self.c) % self.m\n        return self._state\n\n    def generate(self, size):\n        \"\"\"Generates a sequence of integers and \n        normalizes them to [0, 1).\"\"\"\n        sequence_int = []\n        sequence_float = []\n        # Reset state to seed for sequence generation\n        self._state = self.seed \n        \n        for _ in range(size):\n            next_val = self.next_int()\n            sequence_int.append(next_val)\n            # Normalize to a float in [0, 1) by dividing by the modulus\n            sequence_float.append(next_val / self.m)\n            \n        return np.array(sequence_int), np.array(sequence_float)\n\nThe LCG is one of the oldest and best known PRNG which are used to date. As can be seen in the code, it uses three integer parameters \\(a\\), \\(c\\) and the modulo \\(m\\) and computes the next random number using the recurrence:\n\\[\nX_{n+1} = (a X_n + c) \\operatorname{mod} m\n\\tag{2.1}\\]\nStarting at \\(n=0\\), we initialize \\(X_0\\) to the random seed provided.\nWe can now use the generator as follows:\n\n# LCG Parameters (a 'poor' LCG to highlight the deterministic nature)\n# A small modulus (m) leads to a short period and visible patterns.\nSEED = 42\nA = 65  # Multiplier\nC = 1   # Increment\nM = 2**10  # Modulus (1024) - A small M is used for demonstration purposes\nSEQUENCE_SIZE = 100000\n\n# 1. Initialize and Generate Sequence\nprng = LCG(SEED, A, C, M)\nint_sequence, float_sequence = prng.generate(SEQUENCE_SIZE)\nint_sequence[:10]\n\narray([683, 364, 109, 942, 815, 752, 753, 818, 947, 116])\n\n\nWe have now generated 100000 random numbers using LCG (only first 10 are shown). But how can we ensure if this PRNM works well in practice? We will look now at the period length, how to check for uniformity and how to assert if there is serial correlation.\nPeriod length\nThe period length assesses the number of values generated before the sequence of states returns to the first value (the starting state) for the first time. Note that in general the longer, the better. Note that in this case, the maximum possible period is \\(m\\), the modulo of the generator. We can calculate this with a simple Python function as follows:\n\ndef calculate_period(lcg_generator):\n    \"\"\"\n    Calculates the period (cycle length) of the LCG.\n    The period is the number of values generated before the sequence repeats.\n    \"\"\"\n    initial_state = lcg_generator.seed\n    current_state = initial_state\n    \n    # Check for the next state immediately after the seed to start the loop\n    current_state = (lcg_generator.a * current_state + lcg_generator.c) % lcg_generator.m\n    period = 1\n    \n    # Loop until the state returns to the initial seed\n    while current_state != initial_state:\n        current_state = (lcg_generator.a * current_state + lcg_generator.c) % lcg_generator.m\n        period += 1\n        \n        # Safety break for potentially infinite loops in case of a non-standard LCG\n        if period &gt; lcg_generator.m:\n           return f\"Period is greater than modulus m ({lcg_generator.m}). Check parameters.\"\n            \n    return period\n\nperiod = calculate_period(prng)\nperiod\n\n1024\n\n\nSo in this case, our generator reaches the maximum period (1024), which is the best we can do.\nTests for uniformity\nWe want the generated random numbers to be uniformly generated (we will see later how generate numbers with different distributions started with uniformly generated random numbers). For this, we use the \\(\\chi^2\\) test for uniformity:\n\nNull Hypothesis (\\(H_0\\)): The generated numbers are uniformly distributed.\nAlternative Hypothesis (\\(H_1\\)): The generated numbers are not uniformly distributed.\n\nThe main idea of this test is to divide the generated numbers in intervals, and check whether those intervals contain roughly the same number of generated values (e.g. a flat histogram). Like in the classical \\(\\chi^2\\) test, we calculate the expected \\(E_i\\) and the observed \\(O_i\\) frequencies for each range and calculate the \\(\\chi^2\\) statistic as usual:\n\\[\n\\chi^2=\\sum_{i=1}^k\\frac{(O_i-E_i)^2}{E_i}\n\\]\nWe can use the following Python function:\n\ndef chi_squared_uniformity_test(data_float, num_bins=10):\n    \"\"\"\n    Statistical Test: Chi-Squared Goodness-of-Fit Test for Uniformity.\n    \"\"\"\n    N = len(data_float)\n    \n    # 1. Bin the data to get observed frequencies\n    # The bins are equal-sized intervals in [0, 1).\n    observed_frequencies, _ = np.histogram(data_float, bins=num_bins, range=(0, 1))\n    \n    # 2. Calculate expected frequencies for a perfectly uniform distribution\n    expected_frequency = N / num_bins\n    expected_frequencies = np.full(num_bins, expected_frequency)\n    \n    # 3. Perform the Chi-Squared test\n    # The 'chisquare' function compares observed and expected frequencies.\n    # A small p-value (e.g., &lt; 0.05) leads to rejection of H0, meaning non-uniformity.\n    chi2_stat, p_value = chisquare(f_obs=observed_frequencies, f_exp=expected_frequencies)\n    \n    return chi2_stat, p_value, num_bins\n\nchi2_stat, p_value_uniformity, num_bins = chi_squared_uniformity_test(float_sequence)\nprint(f'Chi2 statistic: {chi2_stat}, p-value: {p_value_uniformity}, number of bins: {num_bins}')\n\nChi2 statistic: 2.2074, p-value: 0.9877471315220641, number of bins: 10\n\n\nIn this case, the p-value is much higher than \\(\\alpha=0.05\\) and we cannot reject \\(H_0\\), so the numbers appear to be uniformly random.\nSerial correlation\nThe next possible measure to check is the serial correlation between the numbers generated. The Serial Correlation Check, also known as Autocorrelation at Lag 1, is a diagnostic measure used to characterize and detect a fundamental weakness in simple Pseudorandom Number Generators (PRNGs), such as the Linear Congruential Generator (LCG). The main idea is that the correlation between immediately adjacent numbers (hence lag 1) should be zero.\nTo calculate this, we form two sequences: the generated numbers and the same sequence moved by one place:\n\\[\n\\begin{aligned}\nS_n & = \\{X_1,X_2,X_3,\\dots,X_{n-1}\\} \\\\\nS_{n+1} & = \\{X_2,X_3,X_4,\\dots,X_n\\}\n\\end{aligned}\n\\]\nAnd now we calculate the Pearson correlation coefficient between \\(S_1\\) and \\(S_2\\).\n\\[\nr=\\frac{\\sum (S_1-\\bar{S_1})(S_2-\\bar{S_2})}{\\sqrt{\\sum (S_1-\\bar{S_1})^2(S_2-\\bar{S_2})^2}}\n\\]\nOur goal is that \\(r\\) is as close to zero as possible (note that \\(r\\in[-1,1]\\)). Let’s use the following code:\n\ndef serial_correlation_check(data_float):\n    \"\"\"\n    Characterization: Autocorrelation (Serial Correlation) Check.\n    \"\"\"\n    # X_n: all values except the last one\n    X_n = data_float[:-1]\n    # X_{n+1}: all values except the first one\n    X_n_plus_1 = data_float[1:]\n    \n    # Calculate the Pearson correlation coefficient (r)\n    # The result is an array, we take the correlation between the two sequences (index 0, 1)\n    correlation_matrix = np.corrcoef(X_n, X_n_plus_1)\n    lag_1_correlation = correlation_matrix[0, 1]\n    \n    return lag_1_correlation\n\nlag_1_correlation = serial_correlation_check(float_sequence)\nprint(f'The lag 1 correlation coefficient is {lag_1_correlation}')\n\nThe lag 1 correlation coefficient is 0.008943629579226285\n\n\nWhile the value is low, it’s not as close to zero as it should, which is a known weakness of the LCG (the generated numbers tend to fall onto a number of parallel hyperplanes). This is the reason why PRNM like the LCG are not normally used in practice. The de-facto standard for pseudorandom number generation in practice is the algorithm known as the Mersenne Twister. This is the default generator used in Python or MATLAB, and the preferred one for simulation purposes (but not for cryptographic purposes). The basic idea is to use a highly non-linear twisted generalized feedback shift register. Apart from being much faster than LCG, it passes the serial correlation chek with flying colors:\n\nimport random\n\nrandom.seed(SEED)\n\n# Generate a sequence of random floats in the range [0.0, 1.0)\nfloat_sequence_mt = np.array([random.uniform(0, 1) for _ in range(SEQUENCE_SIZE)])\n\n# Serial Correlation Check\nlag_1_correlation_mt = serial_correlation_check(float_sequence_mt)\nprint(f'The lag 1 correlation coefficient is {lag_1_correlation_mt}')\n\nThe lag 1 correlation coefficient is -0.000962673758206564\n\n\nwhich is an order of magnitude better than the LCG.\n\n\n\n\nBreugel, Boris van, Zhaozhi Qian, and Mihaela van der Schaar. 2023. “Synthetic Data, Real Errors: How (Not) to Publish and Use Synthetic Data.” arXiv. https://doi.org/10.48550/arXiv.2305.09235.\n\n\nJordon, James, Lukasz Szpruch, Florimond Houssiau, Mirko Bottarelli, Giovanni Cherubin, Carsten Maple, Samuel N. Cohen, and Adrian Weller. 2022. “Synthetic Data – What, Why and How?” arXiv. https://doi.org/10.48550/arXiv.2205.03257.\n\n\nOsais, Yahya Esmail. 2017. Computer Simulation: A Foundational Approach Using Python. New York: Chapman; Hall/CRC. https://doi.org/10.1201/9781315120294.",
    "crumbs": [
      "PART I: SIMULATION",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simulation basics</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "10  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Breugel, Boris van, Zhaozhi Qian, and Mihaela van der Schaar. 2023.\n“Synthetic Data, Real Errors: How (Not) to Publish and Use\nSynthetic Data.” arXiv. https://doi.org/10.48550/arXiv.2305.09235.\n\n\nJordon, James, Lukasz Szpruch, Florimond Houssiau, Mirko Bottarelli,\nGiovanni Cherubin, Carsten Maple, Samuel N. Cohen, and Adrian Weller.\n2022. “Synthetic Data – What, Why and How?”\narXiv. https://doi.org/10.48550/arXiv.2205.03257.\n\n\nOsais, Yahya Esmail. 2017. Computer Simulation:\nA Foundational Approach\nUsing Python. New York: Chapman;\nHall/CRC. https://doi.org/10.1201/9781315120294.",
    "crumbs": [
      "References"
    ]
  }
]